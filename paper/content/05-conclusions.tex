% !TEX root = ../main.tex
%
\chapter{Conclusions}
\label{sec:conclusions}

In this thesis we researched the practical feasibility of LLM generation for synthetic online discussions. We created a custom framework supporting automated synthetic discussion creation, annotation and analysis, and explored two different prompting strategies; standard instruction prompting as well as framing the discussion as a competitive, scorable game. We then used this framework to generate three synthetic datasets, containing discussions, annotations by LLM annotators with different SDBs, and controversial comments (according to the LLM annotators) respectively. 

In the context of this research, we used toxicity as a proxy for argument quality. Analyzing the synthetic datasets, we found that the presence of a moderator can be a decisive influence on the toxicity of a discussion. Furthermore, framing the discussion as a scorable game seems to potentially keep LLM users in line using the threat of a moderator whose presence may not be perceivable. Finally, we proved that using different SDBs in LLM annotators yields no significant qualitative difference, and that any difference can be attributed to a change in priors, as opposed to reacting in a different way according to the content and context of the synthetic messages.

Thus, to answer the key research questions posed in the introduction of this thesis, we conclude that LLM self-talking is a realistic and appropriate vector of simulating online discussions. We also found that using SDBs for the LLM users makes the conversations more realistic, and avoids the need for us to explicitly state their stances on various controversial issues. Lastly, LLM annotators do not seem to be affected by the inclusion of SDBs.

