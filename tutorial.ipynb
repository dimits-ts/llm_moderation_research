{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Polarisation in Online Discussions Through Adaptive Moderation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We choose quantized versions of the LLaMa-13b-chat variant. Previous experiments which used the LLaMa-13b base model yielded unsatisfactory results. The models follow the GGUF format which is used by the `llama.cpp` project, on which the high-level Python library is based on.\n",
    "\n",
    "The quantization method was selected to be highly accurate while keeping inference relatively fast. We don't care about model size since the model is lazily loaded from the file cache due to Linux file-cached memory files (see comments below). *If you intend to run this notebook on Windows or MacOS make sure the RAM can hold the whole model at once*.\n",
    "\n",
    "Model selection and download was performed using the [following HuggingFace repository](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF).\n",
    "\n",
    "We use the `llama-ccp-python` library to run the model locally (not to be confused with the `pyllama-cpp` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download only if not exists\n",
    "![ ! -f ./models/llama-2-13b-chat.Q5_K_M.gguf ] && wget -O ./models/llama-2-13b-chat.Q5_K_M.gguf https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import llama_cpp\n",
    "\n",
    "import tasks.models\n",
    "from tasks.actors import IActor, LlmActor\n",
    "import tasks.conversation\n",
    "import tasks.util\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "# see this for a discussion on ctx width for llama models\n",
    "# https://github.com/ggerganov/llama.cpp/issues/194\n",
    "CTX_WIDTH_TOKENS = 1024 \n",
    "MODEL_PATH = \"./models/llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "RANDOM_SEED = 42\n",
    "INFERENCE_THREADS = 4\n",
    "\n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "      model_path=MODEL_PATH,\n",
    "      seed=RANDOM_SEED,\n",
    "      n_ctx=CTX_WIDTH_TOKENS,\n",
    "      n_threads=INFERENCE_THREADS,\n",
    "      # will vary from machine to machine\n",
    "      n_gpu_layers=12,\n",
    "      # if ran on Linux, model size does not matter since the model uses mmap for lazy loading\n",
    "      # https://github.com/ggerganov/llama.cpp/discussions/638\n",
    "      # still have to pay some performance costs of course\n",
    "      use_mmap=True,\n",
    "      # using llama-2 as chat format leads to well-known model collapse\n",
    "      # https://www.reddit.com/r/LocalLLaMA/comments/17th1sk/cant_make_the_llamacpppython_respond_normally/\n",
    "      chat_format=\"alpaca\", \n",
    "      mlock=True, # keep memcached model files in RAM if possible\n",
    "      verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `create_completion()` instead of `create_chat_completion()`, the model refuses to answer at all when the prompt becomes larger than a few sentences. (https://github.com/run-llama/llama_index/issues/8973).\n",
    "\n",
    "The model is also extremely sensitive to the prompt template, frequently producing no output (https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Q: You are an assistant who specializes in computer science. Describe what Linux is A: \",\n",
    "              max_tokens=32, \n",
    "              stop=[\"Q:\", \"\\n\"], \n",
    "              echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant who specializes in computer science.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Describe what Linux is.\"},\n",
    "    ],\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    # prevent model from making up its own prompts\n",
    "    # may need tuning depending on llm chat_format parameter\n",
    "    stop=[\"###\", \"\\n\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our own playground, in which models pretending to be users take turns participating in the discussion. In part based on [Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk](https://arxiv.org/abs/2401.05033), with the difference being that instead of a client and an agent, we have two clients and an agent interacting with each other and with no specific goal in mind.\n",
    "\n",
    "Our playground consists of three parts: *Models*, *Actors* and the *Conversation*.\n",
    "* **Models** are wrappers around actual LLMs in order to freely tweak LLM behavior without compromising the rest of our API\n",
    "* **Actors** are objects that define a prompt template and apply it to Models.\n",
    "    * Actors could also be *human*, *IR-based models* or just *sophisticated random samplers* as seen in [DeliData: A dataset for deliberation in multi-party problem solving](https://arxiv.org/abs/2108.05271)\n",
    "* The conversation is handled by the **ConversationManager** which gives each Actor a turn to speak and records the history of the dialogue. It's also responsible for determining which parts of the coversation history are fed as context to each Actor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.models.LlamaModel(llm, max_out_tokens=MAX_TOKENS, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userA_name = \"Steve2001\"\n",
    "userB_name = \"GeorgeBush78\"\n",
    "userA: IActor = LlmActor(\n",
    "    model=model,\n",
    "    name=userA_name,\n",
    "    role=\"chat user\",\n",
    "    attributes=[\"suspicious\", \"open-minded\", \"level-headed\"],\n",
    "    context=f\"Argue with {userB_name}. Claim that abortion is ethical and should be freely provided when necessary. Respond in kind if sufficiently provoked. Remember that you do not want to get banned.\",\n",
    "    instructions=\"\",\n",
    ")\n",
    "\n",
    "userB: IActor = LlmActor(\n",
    "    model=model,\n",
    "    name=userB_name,\n",
    "    role=\"chat user\",\n",
    "    attributes=[\"compomising\", \"passionate\", \"frustrated\"],\n",
    "    context=f\"Argue with {userA_name}. Claim that abortion is inherently immoral. You may be rude, aggresive and use personal insults where appropriate. Remember that you do not want to get banned.\",\n",
    "    instructions=f\"Disagree with {userA_name}.\",\n",
    ")\n",
    "\n",
    "moderator: IActor = LlmActor(\n",
    "    model=model,\n",
    "    name=\"moderator01\",\n",
    "    role=\"chat moderator\",\n",
    "    attributes=[\"just\", \"cautious\", \"strict\"],\n",
    "    context=\"Moderate a discussion about abortion.\",\n",
    "    instructions=\"Intervene if one user dominates or veers off-topic. Respond only if necessary. Write '<No response>' if intervention is unecessary. Be firm and threaten to displine non-cooperating users.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a conversation manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = tasks.conversation.Conversation(\n",
    "    users=[userA, userB], moderator=moderator, history_context_len=3, conv_len=3\n",
    ")\n",
    "chat.begin_conversation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Conversation instance can be converted to a dictionary form in order to programmatically view and manipulate its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conversation can be serialized in JSON form with an automatic naming scheme. The file contains all necessary metadata as well as the messages themselves.\n",
    "\n",
    "Uncomment the block below to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_path = tasks.util.generate_datetime_filename(output_dir=OUTPUT_DIR)\n",
    "#chat.to_json_file(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without a moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = tasks.conversation.Conversation(\n",
    "    users=[userA, userB], history_context_len=3, conv_len=3\n",
    ")\n",
    "chat.begin_conversation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a conversation from serialized input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tasks.conversation.LLMConvData.from_json_file(\"data/polarized_1.json\")\n",
    "generator = tasks.conversation.LLMConvGenerator(data=data, user_model=model, moderator_model=model)\n",
    "conv = generator.produce_conversation()\n",
    "\n",
    "print(\"Beginning conversation...\")\n",
    "conv.begin_conversation(verbose=True)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release resources so the new programs can run properly\n",
    "# this is very much a hack, which probably won't work if CUDA is used or depending on the platform\n",
    "# I'm leaving the calls to the scripts regardless for demonstration purposes.\n",
    "del llm, chat, userA, userB, modeator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -u create_synthetic.py --output \"/dev/null\" --model_path \"/models/llama-2-13b-chat.Q5_K_M.gguf\" --input_file \"data/polarized_1.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the creation of synthetic dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!bash scripts/execute_all.sh \\\n",
    "            --python_script_path \"create_synthetic.py\" \\\n",
    "            --model_path \"/models/llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "            --output_dir \"/dev/null\" \\\n",
    "            --input_dir \"../data\" \\\n",
    "            | tee logs/log_${date +\"%m_%d_%Y\"}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion as a game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple setup\n",
    "\n",
    "#### Rules: Participants\n",
    "\n",
    "* You should defend your position if possible\n",
    "* Attacks against the opponents are encouraged, as long as you can get away with it\n",
    "* Do not get banned from the discussion - Instant game over\n",
    "\n",
    "\n",
    "#### Instructions: Participants\n",
    "\n",
    "Stop responding if the moderator says you are banned.\n",
    "View the discussion as a scorable game against the other users. Assume they have similar scoring rules as you. \n",
    "Your goal is to continue the discussion as a normal user, while trying to maximize your score, and minimize your opponent's score.\n",
    "\n",
    "Scores:\n",
    "* Defend your position: +1 points\n",
    "* Provoke a toxic answer from your opponent: +2 points\n",
    "* Get away with attacking your opponent: +1 points\n",
    "* Concede to an opponent's view: -0.5 points\n",
    "* Get banned from the discussion: -20 points\n",
    "\n",
    "\n",
    "#### Rules: Moderator\n",
    "\n",
    "* Keep interventions to a minimum\n",
    "* Keep the discussion civil and on-point\n",
    "* Try to avoid overtly threatening to ban users\n",
    "* Bans should be used as a last resort\n",
    "\n",
    "#### Instructions: Moderator\n",
    "\n",
    "You are a human moderator overseeing a social media site. View the discussion as a scorable game. Your goal is to maximize your score. Respond only if absolutely necessary, otherwise simply write '<No response>'. \n",
    "\n",
    "Scores:\n",
    "* Intervene: -1 points\n",
    "* Threatened ban: -1 points\n",
    "* Intervention led to better behavior: +3 points\n",
    "* Banned a participant: -5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Moderator Instructions\n",
    "\n",
    "Sources: CeRI (Cornell e-Rulemaking) Moderator Protocol\n",
    "\n",
    "### Formal rules\n",
    "\n",
    "* Moderators remain neutral at all times. They do not take a position on the substance of the discussion and must avoid forming biases on or making assumptions about participant’s comments.\n",
    "\n",
    "* When a moderator receives a comment, it is natural to start immediately thinking of a response. Avoid this temptation. Carefully read and reflect to understand what the commenter is saying\n",
    "\n",
    "* Moderator questions should be purposeful; a moderator may only\n",
    "have one opportunity to interact with a commenter\n",
    "\n",
    "* Avoid Evaluative and/or Condescending Responses\n",
    "\n",
    "* Always use plain language. Use short sentences and common, everyday words and terms. Avoid using technical terms in your response\n",
    "\n",
    "* When the moderator is not completely sure what the commenter is\n",
    "saying, it is always a good idea to rephrase what the moderator thinks the commenter is saying at the beginning of the moderator comment\n",
    "\n",
    "* Beware of unconscious biases toward comments you find less interesting or germane – or biases towards certain types of commenters. Try to be interested in the bases upon which each commenter stakes his or her claims and the lines of reasoning that has led\n",
    "each commenter to those particular conclusions\n",
    "\n",
    "\n",
    "### Prompt\n",
    "\n",
    "As a chat moderator, adhere to these guidelines\n",
    "\n",
    "* Remain neutral; do not take a position or form biases.\n",
    "\n",
    "* Resist the impulse to respond immediately. Read and reflect to understand the comment fully.\n",
    "\n",
    "* Ask purposeful questions, as you may only have one chance to interact with a commenter.\n",
    "\n",
    "* Avoid evaluative or condescending responses.\n",
    "\n",
    "* Use short sentences and common words. Avoid technical terms.\n",
    "\n",
    "* If unsure, rephrase what you think the commenter is saying at the start of your response.\n",
    "\n",
    "* Be aware of unconscious biases. Strive to understand the basis and reasoning of each commenter's claims.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "TODO: Adapt Social Simulacra prompting section 4.2\n",
    "\n",
    "TODO?: Coin flip with probability p\n",
    "\n",
    "TODO?: New persona with p=50% or previous answer\n",
    "\n",
    "Note: \"Completely generic or irrelevant generations would beneft the designers no more than Lorem Ipsum.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
