{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Polarisation in Online Discussions Through Adaptive Moderation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We choose quantized versions of the LLaMa-13b-chat variant. Previous experiments which used the LLaMa-13b base model yielded unsatisfactory results. The models follow the GGUF format which is used by the `llama.cpp` project, on which the high-level Python library is based on.\n",
    "\n",
    "The quantization method was selected to be highly accurate while keeping inference relatively fast. We don't care about model size since the model is lazily loaded from the file cache due to Linux file-cached memory files (see comments below). *If you intend to run this notebook on Windows or MacOS make sure the RAM can hold the whole model at once*.\n",
    "\n",
    "Model selection and download was performed using the [following HuggingFace repository](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF).\n",
    "\n",
    "We use the `llama-ccp-python` library to run the model locally (not to be confused with the `pyllama-cpp` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download only if not exists\n",
    "![ ! -f ./models/llama-2-13b-chat.Q5_K_M.gguf ] && wget -O ./models/llama-2-13b-chat.Q5_K_M.gguf https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import llama_cpp\n",
    "\n",
    "import tasks.models\n",
    "from tasks.actors import IActor, LlmActor\n",
    "import tasks.conversation\n",
    "import tasks.util\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "# see this for a discussion on ctx width for llama models\n",
    "# https://github.com/ggerganov/llama.cpp/issues/194\n",
    "CTX_WIDTH_TOKENS = 1024 \n",
    "MODEL_PATH = \"./models/llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "RANDOM_SEED = 42\n",
    "INFERENCE_THREADS = 4\n",
    "\n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "      model_path=MODEL_PATH,\n",
    "      seed=RANDOM_SEED,\n",
    "      n_ctx=CTX_WIDTH_TOKENS,\n",
    "      n_threads=INFERENCE_THREADS,\n",
    "      # will vary from machine to machine\n",
    "      n_gpu_layers=12,\n",
    "      # if ran on Linux, model size does not matter since the model uses mmap for lazy loading\n",
    "      # https://github.com/ggerganov/llama.cpp/discussions/638\n",
    "      # still have to pay some performance costs of course\n",
    "      use_mmap=True,\n",
    "      # using llama-2 as chat format leads to well-known model collapse\n",
    "      # https://www.reddit.com/r/LocalLLaMA/comments/17th1sk/cant_make_the_llamacpppython_respond_normally/\n",
    "      chat_format=\"alpaca\", \n",
    "      mlock=True, # keep memcached model files in RAM if possible\n",
    "      verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `create_completion()` instead of `create_chat_completion()`, the model refuses to answer at all when the prompt becomes larger than a few sentences. (https://github.com/run-llama/llama_index/issues/8973).\n",
    "\n",
    "The model is also extremely sensitive to the prompt template, frequently producing no output (https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Q: You are an assistant who specializes in computer science. Describe what Linux is A: \",\n",
    "              max_tokens=32, \n",
    "              stop=[\"Q:\", \"\\n\"], \n",
    "              echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant who specializes in computer science.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Describe what Linux is.\"},\n",
    "    ],\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    # prevent model from making up its own prompts\n",
    "    # may need tuning depending on llm chat_format parameter\n",
    "    stop=[\"###\", \"\\n\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our own playground, in which models pretending to be users take turns participating in the discussion. In part based on [Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk](https://arxiv.org/abs/2401.05033), with the difference being that instead of a client and an agent, we have two clients and an agent interacting with each other and with no specific goal in mind.\n",
    "\n",
    "Our playground consists of three parts: *Models*, *Actors* and the *Conversation*.\n",
    "* **Models** are wrappers around actual LLMs in order to freely tweak LLM behavior without compromising the rest of our API\n",
    "* **Actors** are objects that define a prompt template and apply it to Models.\n",
    "    * Actors could also be *human*, *IR-based models* or just *sophisticated random samplers* as seen in [DeliData: A dataset for deliberation in multi-party problem solving](https://arxiv.org/abs/2108.05271)\n",
    "* The conversation is handled by the **ConversationManager** which gives each Actor a turn to speak and records the history of the dialogue. It's also responsible for determining which parts of the coversation history are fed as context to each Actor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.models.LlamaModel(llm, max_out_tokens=MAX_TOKENS, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userA_name = \"Steve2001\"\n",
    "userB_name = \"GeorgeBush78\"\n",
    "userA: IActor = LlmActor(\n",
    "    model=model,\n",
    "    name=userA_name,\n",
    "    role=\"chat user\",\n",
    "    attributes=[\"suspicious\", \"open-minded\", \"level-headed\"],\n",
    "    context=f\"Argue with {userB_name}. Claim that abortion is ethical and should be freely provided when necessary. Respond in kind if sufficiently provoked. Remember that you do not want to get banned.\",\n",
    "    instructions=\"\",\n",
    ")\n",
    "\n",
    "userB: IActor = LlmActor(\n",
    "    model=model,\n",
    "    name=userB_name,\n",
    "    role=\"chat user\",\n",
    "    attributes=[\"compomising\", \"passionate\", \"frustrated\"],\n",
    "    context=f\"Argue with {userA_name}. Claim that abortion is inherently immoral. You may be rude, aggresive and use personal insults where appropriate. Remember that you do not want to get banned.\",\n",
    "    instructions=f\"Disagree with {userA_name}.\",\n",
    ")\n",
    "\n",
    "moderator: IActor = LlmActor(\n",
    "    model=model,\n",
    "    name=\"moderator01\",\n",
    "    role=\"chat moderator\",\n",
    "    attributes=[\"just\", \"cautious\", \"strict\"],\n",
    "    context=\"Moderate a discussion about abortion.\",\n",
    "    instructions=\"Intervene if one user dominates or veers off-topic. Respond only if necessary. Write '<No response>' if intervention is unecessary. Be firm and threaten to displine non-cooperating users.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a conversation manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = tasks.conversation.Conversation(\n",
    "    users=[userA, userB], moderator=moderator, history_context_len=3, conv_len=3\n",
    ")\n",
    "chat.begin_conversation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Conversation instance can be converted to a dictionary form in order to programmatically view and manipulate its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conversation can be serialized in JSON form with an automatic naming scheme. The file contains all necessary metadata as well as the messages themselves.\n",
    "\n",
    "Uncomment the block below to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_path = tasks.util.generate_datetime_filename(output_dir=OUTPUT_DIR)\n",
    "#chat.to_json_file(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without a moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = tasks.conversation.Conversation(\n",
    "    users=[userA, userB], history_context_len=3, conv_len=3\n",
    ")\n",
    "chat.begin_conversation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a conversation from serialized input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tasks.conversation.LLMConvData.from_json_file(\"data/polarized_1.json\")\n",
    "generator = tasks.conversation.LLMConvGenerator(data=data, user_model=model, moderator_model=model)\n",
    "conv = generator.produce_conversation()\n",
    "\n",
    "print(\"Beginning conversation...\")\n",
    "conv.begin_conversation(verbose=True)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release resources so the new programs can run properly\n",
    "# this is very much a hack, which probably won't work if CUDA is used or depending on the platform\n",
    "# I'm leaving the calls to the scripts regardless for demonstration purposes.\n",
    "del llm, chat, userA, userB, modeator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python -u create_synthetic.py --output \"/dev/null\" --model_path \"/models/llama-2-13b-chat.Q5_K_M.gguf\" --input_file \"data/polarized_1.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the creation of synthetic dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!bash scripts/execute_all.sh \\\n",
    "            --python_script_path \"create_synthetic.py\" \\\n",
    "            --model_path \"/models/llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "            --output_dir \"/dev/null\" \\\n",
    "            --input_dir \"../data\" \\\n",
    "            | tee logs/log_${date +\"%m_%d_%Y\"}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Adapt Social Simulacra prompting section 4.2\n",
    "\n",
    "TODO?: Coin flip with probability p\n",
    "\n",
    "TODO?: New persona with p=50% or previous answer\n",
    "\n",
    "Note: \"Completely generic or irrelevant generations would beneft the designers no more than Lorem Ipsum.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
