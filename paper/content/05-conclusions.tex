% !TEX root = ../main.tex
%
\chapter{Conclusions}
\label{sec:conclusions}

In this thesis we researched the practical feasibility of LLM generation for synthetic online discussions. We created a custom framework supporting automated synthetic discussion, annotation and analysis, and explored two different prompting strategies; vanilla instruction prompting as well as framing the discussion as a competitive, scorable game. We then used this framework to generate three synthetic datasets, containing discussions, annotations by LLM annotators with different SDBs, and controversial comments respectively. 

In the context of this research, we used toxicity as a proxy for argument quality. Analyzing the synthetic dataset we found that the presence of a moderator can be a decisive influence on the toxicity of a discussion. Furthermore, framing the discussion as a scorable game seems to potentially keep LLM users in line using the threat of a moderator whose presence may not be perceivable. Finally, we proved that using different SDBs in LLM annotators yields no significant qualitative difference, and that any difference can be attributed to a change in priors, as opposed to reacting in a different way according to the content and context of the synthetic messages.

