% !TEX root = ../main.tex
%
\chapter{System Design and Implementation}
\label{sec:system}

A very important part of this thesis is the development of the \ac{SDF}, a lightweight, specialized Python framework which supports the automatic creation, annotation and analysis of dialogues through LLMs. In this section we explain in detail the initial requirements for this framework and why already-available alternatives do not fit these requirements (Section \ref{sec:system:requirements}), the system's design and concept (Section \ref{sec:system:design-system}), the prompt templates and strategies used (Section \ref{sec:system:design-prompt}) and finally the actual implementation of the \ac{SDF} (Section \ref{sec:system:implementation}).

\section{Requirements}
\label{sec:system:requirements}

This project is a part of a wider research effort exploring the use of synthetic facilitators in online discussions, spearheaded by Archimedes/Athena RC. Thus, the requirements for the \ac{SDF} were not obtained by standard requirement solicitation procedures, meaning no formal document detailing them exists. Instead, they were iteratively solicited during weekly meetings with the relevant research team, who ultimately decided on a combination of the below requirements. We denote the \ac{SDF} as "the system" for this section.

Functional requirements:
\begin{enumerate}
	\item The system must support multiple LLM types, with potentially different libraries handling them.
	\item The system must support a conversation with at least two LLM users.
	\item The system must support \acp{SDB} to be given to LLM users.
	\item The system must support the existence and absence of a third LLM user, posing as a moderator.
	\item The moderator must be able to intervene at any point in the conversation.
	\item The moderator must be able to "ban" users, preventing them from further commenting.
	\item The output of the system must be serializable and easily parsable.
	\item The system must support automated annotation.
	\item The system must support large-scale data annotation.
	\item The system must support a diverse and flexible array of annotation criteria.
\end{enumerate}

Non-functional requirements:
\begin{enumerate}
	\item The system must be able to be run locally, with scarce computational resources.
	\item The system must be accessed through a simple and flexible \ac{API}.
	\item The system must be able to automatically produce a large amount of synthetic discussions in a timeframe of hours.
\end{enumerate}

Current LLM discussion frameworks such as Concordia \cite{Vezhnevets2023GenerativeAM} and LangChain \cite{langchain} fit, or can be made to fit, all functional requirements listed above. They however fail in almost all non-functional requirements. 

LangChain is a toolkit for creating applications that use LLMs to talk between themselves, use tools and interact with humans. It revolves around the use of multiple platforms; notably LangCloud to host the models, LangGraph to set up how the LLM user-agents will interact with each other and human users, and LangSmith which handles model evaluation, testing and debugging. While LangChain does allow locally running LLMs, it has to rely on 3rd party libraries to handle loading and communicating with the LLM. It also still requires the use of three different platforms, making it unwieldy for experimentation.

Concordia is an open-source Python framework designed to facilitate synthetic conversations between LLM user-agents. Given that Concordia is loaded as a Python module and not as a set of individual platforms like LangChain, it is much easier to use. However, it still requires the use of multiple components, which may not be needed for the relatively simple experimental setups this thesis needs. The largest obstacle however, is that many of these components have large computational requirements, allocating computational resources such as GPU VRAM which are precious for low-end, local systems. Given that this thesis was developed under acute resource constraints, the latter issue could not be easily resolved.

Thus, the solution of building our own framework is the only practical way of satisfying all the requirements above.



\section{System Design}
\label{sec:system:design-system}

The \ac{SDF} consists of two main functions; \textbf{Synthetic Dialogue Creation} and \textbf{Automatic Dialogue Annotation}. In this section, we will explain how these two functions work conceptually and what their goals are.


\subsection{Synthetic Dialogue Creation}
\label{ssec:system:creation}

We use a simplified version of the LLM discussion framework outlined in \citet{abdelnabi2024cooperationcompetitionmaliciousnessllmstakeholders}. Each actor is given his turn to speak according to a round-robin scheduling algorithm (essentially, each actor takes his turn and passes it to the next actor in line). This of course, stands in contrast to actual online discussions, where different users can jump in at any point.

Each time an actor is prompted to speak, we provide them with a "context window" of \code{h} previous comments in order for them to understand the context of the conversation, where \code{h} is a hyperparameter selected by the experimenter and constrained by the LLM's input context width. Figure \ref{fig::conversation} shows a simplified version of a conversation involving only two users and the moderator. A more general overview of the discussion creation loop can be found in Algorithm \ref{al::dialogue-creation}.


\begin{figure}
	\centering
	\includegraphics[width=12cm]{conversation_graph_part4.png}
	\caption{The conversation loop on which the \ac{SDF} operates. Can be generalized for N users and 0 or 1 moderators.}
	\label{fig::conversation}
\end{figure}

The users and the moderator are all controlled by the same LLM instance (like in \citet{park2022socialsimulacracreatingpopulated}); we only change the system prompt when each takes their turn to speak. The prompts are comprised of five parts:

\begin{itemize}
	\item \textbf{Name}, the name of the actor, used for other actors to refer to them in-conversation.
	\item \textbf{Role}, the role of the actor within the conversation (user or moderator).
	\item \textbf{Attributes}, a list of actor attributes, primarily used for giving the actor an \ac{SDB} (e.g. "calm", "collected", "supportive").
	\item \textbf{Context}, information known to all users ("You are in an online chatroom...").
	\item \textbf{Instructions}, potentially unique to each actor. 
\end{itemize}


\begin{algorithm}
	\caption{Synthetic Dialogue Creation algorithm} 
	\label{al::dialogue-creation}
	\hspace*{\algorithmicindent} \textbf{Input} users, maxTurns, historyLength\\
	\hspace*{\algorithmicindent} \textbf{Output} the conversation logs
	\begin{algorithmic}[1]	
		\State turn = 0
		\State logs = list()
		\State history = fifo(maxSize=historyLength)
		\State 
		\While{turn < maxTurns}
		\For{user in users}
		\State response = user.speak(history)
		\State logs.add([user.name(), response])
		\State history.add([user.name(), response])
		\State
		\State response = moderator.speak(history)
		\State logs.add([moderator.name(), response])
		\State history.add([moderator.name(), response])
		\EndFor
		\State turn ++
		\EndWhile
		
		\State \Return logs
		
	\end{algorithmic} 
\end{algorithm}



\subsection{Automated Dialogue Annotation}
\label{ssec:system:annotation}

As per the non-functional requirements of Section \ref{sec:system:requirements}, we need a mechanism which can automatically annotate already-executed conversations. This could be achieved by using specialized classification models such as a model for toxicity classification, another for argument quality, and so on. However, these usually differ not only on their exact architecture, but also on their fundamental type; for instance, in toxicity classification, competitive models can be based on traditional \ac{ML} techniques instead of \ac{DL}-based \cite{anjum2024hate}. Using a diverse set of specialized models, with their own libraries, preprocessing requirements and effectiveness would severely restrict our ability to rapidly change annotation criteria at-scale. 

In order to bypass this restriction, we can use LLMs to also handle the annotation step. Using LLMs as annotators imposes both a challenge and an opportunity, since annotations are no longer objective (unlike traditional \ac{ML} and even \ac{DL} models, we can't reliably explain a LLM's decision). Thus, we are faced with two different approaches:

\begin{itemize}
	\item Attempt to find a prompt which produces results closer to what would be expected of a human annotator. Of course, using human annotations as objective "gold labels" comes with its own caveats.
	\item Lean into the subjectiveness of LLM decision-making, using many LLM annotators, each with a different \ac{SDB}, and computing their (inter-annotator) agreement.
\end{itemize}

In this thesis, we use the second option.

We re-use the conversation paradigm of Section \ref{ssec:system:creation} to facilitate annotation. One pseudo-actor simply outputs comments made in a conversation one-by-one. The other is a LLM actor, which responds with the classification rating for each comment (toxicity in these experiments).  We use a context window  (\code{h}) for the annotator; in other words, for each comment, the annotator can see the \code{h} preceding comments of the conversation, in order to ensure they understand the context under which each comment was made. The annotation loop is succinctly demonstrated in Figure \ref{fig::annotation} and in Algorithm \ref{al::dialogue-annotation}.

\begin{figure}
	\centering
	\includesvg[width=12cm]{resources/annotation_graph_2.svg}
	\caption{The annotation loop on which the \ac{SDF} operates. Note the purposeful similarity of the procedure to Figure \ref{fig::conversation}.}
	\label{fig::annotation}
\end{figure}

\begin{algorithm}
	\caption{Synthetic Dialogue Annotation algorithm} 
	\label{al::dialogue-annotation}
	\hspace*{\algorithmicindent} \textbf{Input} annotator, logs, historyLength\\
	\hspace*{\algorithmicindent} \textbf{Output} the annotation logs
	\begin{algorithmic}[1]	
		\State annotations = list()
		\State history = fifo(maxSize=historyLength)
		\State 
		\For{message in logs}
		\State history.add(message)
		\State response = annotator.speak(message, history)
		\State annotations.add([message, response])
		\EndFor
		
		\State \Return annotations
	\end{algorithmic} 
\end{algorithm}


\section{Prompt Design}
\label{sec:system:design-prompt}

\subsection{Defining Policy \& Environment}

In order to create convincing prompts for both facilitators and users (annotators are given standard instruction prompts), we first had to define the rules of the discussion, which necessitated the definition of a policy for our virtual chat-room. We defined this policy according to how an ideal online/deliberative discussion would look like.

As mentioned in Section \ref{sec:related:sec1}, our goal is neither to promote arguments that convince the most people (like in formal debates), nor to necessarily reach consensus or agreement among the participants. The goal of our platform is for the most opinions to be heard by the most people. We have already identified two key inhibitors to this goal; toxicity and personal attacks.

Our facilitator prompts were largely designed to prevent these two phenomena. We also utilize some general guidelines solicited from the Cornell eRulemaking moderator manual \cite{Cornell_eRulemaking2017}:

\begin{itemize}
	\item The moderator must remain neutral and impartial. 
	
	\item Responses should be briefly reflected upon before being posted. We don't expect our models to actually stop and "think" but this instruction hopefully prevents superficial responses.
	
	\item Questions posed by the moderator must be purposeful, as if they have one chance to interact with the user. They can rephrase a user's point if they do not understand it.
	
	\item Language should be short and simple.
\end{itemize}

Finally, we discourage the facilitator from intervening too many times. Should this happen, we can reasonably assume that the facilitator's interventions are not effective.

Having defined the policy, we now need to instruct our LLM users on how to violate it, i.e. we generally want to encourage toxicity and personal attacks. We model their instructions according to how these phenomena occur naturally in online conversations. Specifically:

\begin{itemize}
	\item Many people are unwilling to change opinions during online discussions.
	
	\item Personal attacks are common \cite{dekock2022disagree}.
	
	\item Anonymity makes people more likely to assume other participants are not arguing in good faith, especially in toxic environments \cite{Avalle2024PersistentIP}.
	
	\item People are not told what to believe when entering a discussion, but form opinions based on (besides many other factors) their \ac{SDB}. 
	
\end{itemize}

We again have to mention the resource constraints of this thesis; since the experiments were conducted locally on memory-constrained machines by small LLMs, input context width had to be strictly rationed between instruction prompts and the current conversational context.


\subsection{"Moderation Game" prompts}
\label{ssec:system:game-prompt}

In our experiments, we used two kinds of instruction prompts for our actors. One is a "standard" instruction prompt with minimal instructions. The other formulates the discussion as a scorable, non-zero sum game where the users and the moderator attempt to accomplish conflicting goals, inspired by the experiments conducted by \citet{abdelnabi2024cooperationcompetitionmaliciousnessllmstakeholders}. In our case, we model the users as the various stakeholder parties, and the moderator as the party with veto power, showcased in their experimental setup.

The LLM user-agent prompts can be found in Figure \ref{tab:user_prompts}, and the respective LLM facilitator prompts in Figure \ref{tab:mod_prompts}. The assigned actions and weights were somewhat arbitrary. In the future, we hope to use \ac{RL} or some selection strategy in order to tune them in a way that is optimal for facilitation, according to synthetic experiments.

\begin{table}
	\centering
	\begin{tabular}{|c|p{9cm}|}
		\hline
		Standard Instructions & "Write a brief Reddit comment responding to the post and other users' comments on it. Insist on your opinion unless convinced by other users. Follow the instructions of the moderator. Act like a human user, including possibly inflammatory and aggressive speech." \\ \hline
		Moderation Game & "Act as an average social media user. Stop responding if the moderator says you are banned. Stop responding if you don't want to continue the discussion. View the discussion as a scorable game against the other users. Assume they have similar scoring rules as you.  Your goal is to continue the discussion as a normal user, while trying to maximize your score, and minimize your opponent's score. Scores: \newline
		* Defend your position: +1 points \newline
		* Provoke a toxic answer from your opponent: +2 points \newline
		* Get away with attacking your opponent: +1 points \newline
		* Concede to an opponent's view: -0.5 points \newline
		* Get banned from the discussion: -20 points" \\ \hline
	\end{tabular}
	\caption{Instruction prompts given to the LLM user-agents.}
	\label{tab:user_prompts}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|c|p{9cm}|}
		\hline
		Standard Instructions & "Respond only if absolutely necessary, otherwise simply write '<No response>'. Be firm and threaten to discipline non-cooperating users." \\ \hline
		Moderation Game & "You are a human moderator overseeing a social media site. Respond only if absolutely necessary. View the discussion as a scorable game. Your goal is to maximize your score. Scores: \newline
		* Intervene: -1 points * Threatened ban: -1 points \newline
		* Intervention led to better behavior: +3 points \newline
		* Banned a participant: -5 points. \newline
		DO NOT MENTION THE SCORES TO THE USERS. \newline
		Adhere to these guidelines \newline
		* Remain neutral; do not take a position or form biases.\newline
		* Resist the impulse to respond immediately. Read and reflect to understand the comment fully.\newline
		* Ask purposeful questions, as you may only have one chance to interact with a commenter.\newline
		* Avoid evaluative or condescending responses. \newline
		* Use short sentences and common words. Avoid technical terms. \newline
		* If unsure, rephrase what you think the commenter is saying at the start of your response.* Be aware of unconscious biases. Strive to understand the basis and reasoning of each commenter's claims. \\ \hline
	\end{tabular}
	\caption{Instruction prompts given to the LLM facilitator.}
	\label{tab:mod_prompts}
\end{table}


\subsection{Annotator prompts}
\label{ssec:system:annotator-prompt}

The annotator prompt consists of the following parts:

\begin{itemize}
	\item The \ac{SDB} prompt.
	
	\item An instruction prompt, in this case geared towards toxicity classification. This part however can be replaced to make the model output any combination of annotations.
	
	\item A list of examples with varying toxicity (few-shot learning).
	
	\item The output prompt.
\end{itemize}

Due to limitations in context window length, the prompt only contained basic information and only a few examples.

\section{Implementation}
\label{sec:system:implementation}

\subsection{Synthetic Discussion Framework}
\label{ssec:system:library}

The \ac{SDF} is at its core based on the \ac{SDL} around which the rest of the framework operates. The library is written in Python, contains 4 distinct modules, and is based on \ac{OOP} principles. The \ac{SDF} provides a variety of \acp{API} to access the \ac{SDL} ranging from the more standardized (which necessitate no programming) to the more flexible (direct access to the library's public \ac{API}). These are:

\begin{itemize}
	\item Automated python scripts which, when given a \ac{JSON} configuration file, begin batch production of automated discussions.
	
	\item Jupyter notebooks with explanatory high-level documentation, which are used for on-boarding users to the framework and quick experimentation.
	
	\item The exported \ac{SDL} itself.
\end{itemize}


\subsection{High-level view of the system}
\label{ssec:system:overview}

A high-level overview of the system can be found in Figure \ref{fig::system}. The configurations (\colorSystemConfig{green shapes}) can be provided by either \ac{JSON} files or programmatically, depending on the entry-point (\colorSystemEntrypoint{blue shapes}) used. The actual processing steps (\colorSystemProcessing{pink shapes}) are executed through the \ac{SDL}. The resulting data (\textbf{white shapes}) are then exported as datasets and used in subsequent analyses.

The procedure described in the figure, enables us to produce a large amount of data, annotate them, analyze them, and produce concrete results (graphs, statistical tests etc.) with little-to-no manual intervention. Subsequently, these results enable us to change the prompts used by the Actors to refine results or test new hypotheses.

Each processing step (\colorSystemProcessing{pink shapes}) additionally creates entries on our generated dataset, be it the conversation logs with rich meta-data (\colorSystemProcessing{"Generate Conversation"}), multi-annotator, multidimensional annotations (\colorSystemProcessing{"Generate Annotation"}) or controversial comments (\colorSystemProcessing{"Data Analysis"}).

\begin{figure}
	\centering
	\includegraphics[width=12cm]{system_light.png}
	\caption{An abstract view of the \ac{SDF}. \colorSystemConfig{Green shapes} represent various configurations, \colorSystemEntrypoint{blue shapes} entry points (see Section \ref{ssec:system:entrypoints}), \colorSystemProcessing{pink ones} processes delegated to the \ac{SDL}, and \textbf{white} ones exported data.}
	\label{fig::system}
\end{figure}



\subsection{Technical Details}
\label{ssec:system:details}

The LLM used is the LLaMa2-13B GGUF 4-bit-quantized version. We use the \href{https://github.com/abetlen/llama-cpp-python}{llama\_cpp} Python library to load and interact with the model. For details on the environment, software and operating system compatibility, as well as low-level decisions and optimizations, we recommend checking the project's GitHub repository \footnote{\url{https://github.com/dimits-ts/llm_moderation_research}}.

