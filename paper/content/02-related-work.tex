% !TEX root = ../main.tex
%
\chapter{Background and Related Work}
\label{sec:related}

This thesis intersects several scientific domains, including \ac{NLP}, Social Sciences, Data Science, and Computer Science. Our focus lies particularly on discourse facilitation, argument quality, and conversational dynamics within Social Sciences, alongside the application of Large Language Models (LLMs) in synthetic dialogue generation, an area of interest within \ac{NLP}. This section provides a focused review of these topics, detailing the rationale behind the selection of methodologies employed in the creation of our framework, and subsequent analyses. 

We first provide a basic background for human argumentation and the capabilities of LLMs (Section \ref{sec:related:sec1}), and then dive into what has been achieved in the field of synthetic dialogue generation and discourse facilitation (Section \ref{sec:related:sec2}).


\section{Background}
\label{sec:related:sec1}

\subsection{How and why do humans argue?}
\label{sec:background:arguments-how}

Collective deliberation and decision-making has been long hypothesized, and proven, to yield better results than those performed by individuals \cite{david-collaborative, stefan-dissent}. This idea has often been expressed by the phrase "the group is better than the sum of its parts". 

Social science research often attempts to categorize distinct tactics in arguments. \citet{graham2008disagree} proposes a hierarchy of disagreements, ranging from name-calling, to refuting the central point of an argument.  While a convenient framework, it has not been verified empirically \cite{dekock2022disagree}. \citet{walker-etal-2012-corpus} attempt to create a hierarchy of emotional vs rational responses, highlighting that debating is not a one-dimensional series of rebuttals, but also contains attempts at negotiation and resolution. It however disregards the fact that an argument can be both factual and emotional \cite{dekock2022disagree}. There are many attempts at refining the original hierarchy, such as the one presented by \citet{benesch2016counterspeech}.

Disagreements and toxicity are a natural part of human dialogue, which however often lead to the discussion failing. \citet{dekock2022disagree} demonstrate that personal attacks may lead to a positive feedback loop where once a personal attack has been issued, it is very likely that another will be issued both by the same person and/or by another participant in the future, often leading to communication breaking down. Thus, effective moderation may be contingent on cracking down on personal attacks from the very start, or completely dissuading participants from using them altogether. However, recent studies suggest this may not be the case. \citet{Avalle2024PersistentIP} show that, from examining data of over the last 30 years, toxicity does not seem to discourage participation or escalate disagreements. Non-verbal discussions (newspaper comment sections, online discussions e.t.c.) nevertheless frequently cause participants to entrench themselves in their own beliefs, believing that the other participants are hostile to them, when exposed to toxic language.

\subsection{The characteristics of online discussions}
\label{sec:background:arguments-online}

The above observation may lead us to conclude that online conversations differ greatly from offline (face-to-face) conversations. Online forums are typically larger in terms of length and number of participants, forming large trees of replies leading back to an Original Post (OP) \cite{boschi2021wordunderstandingsampleonline}. Real-time-chats, in the form of Internet Relay Chats (IRC) usually don't follow this tree paradigm, however. Both have a fundamental issue; the large amount of information being shared means that the participants need to sample the discussion effectively, usually leading to misinterpretations, low-quality conversational context, and user fatigue \cite{boschi2021wordunderstandingsampleonline}. 

Additionally, online conversations are often overseen by \textit{moderators}, people appointed to oversee discussions with the clear purpose of observing that they are conducted in an orderly and fair manner. Some of their principal assignments are related to decorum, enforcement of guidelines, and addressing any issue that may arise during the course of the proceedings. In informal communities, respected members of the community usually assume the role of moderator, while in more formal settings, the role may be assigned to paid employees. In both cases, but especially the latter, moderators are given a set of special rules and guidelines to follow; these often include being neutral, impartial, understanding, firm, and to provide information on the discussion, community and their own responsibilities and limitations \cite{Cornell_eRulemaking2017}.

Besides moderators, formal online discussion platforms and communities (as well as educational institutions \cite{Wang2008StudentfacilitatorsRI, Zhong2019ExploringTR}) employ the use of discourse \textit{facilitators}. While a moderator oversees discussions with the specific purpose of directing conversation towards predefined goals, a facilitator focuses on creating an environment that fosters collaboration and engagement among participants, thereby encouraging them to collectively achieve new insights or solutions \cite{wef_moderation}. The differences between the two are subtle, and the terms are often used interchangeably, or as a combination of both duties, in practice (such as in \citet{Zhong2019ExploringTR, Carson2008}). We will thus use the term "facilitation" and "moderation" as to mean both responsibilities, as outlined above. 

\subsection{What makes a good argument?}
\label{sec:background:good-argument}

Both in popular perception and in academia, the best arguments are often considered to be the ones that sway public opinion, or that force the opposing side to concede previously held talking points. For instance, while the research of \citet{zhang2016-oxford} claim to investigate how ideas flow between groups holding and discussing different views, and while their insights are doubtlessly important, the authors end up investigating what wins an argument, and their analysis quickly pivots to audience reactions, votes, rhetorical dominance and predictive modeling for which team is likely to win a debate, instead of how ideas influence the discussion itself. Thus, they ultimately miss their stated goal.

We can not allow this notion of the best argument being the one that convinces the most people leak through when designing systems around deliberative and/or general online discussions. Should this happen, the culture of the platform will be one of highly-opinionated, heated discourse, between stubborn participants who try to "win" discussions by any means necessary. This phenomenon is mentioned by \citet{karadzhov2023delidata}, who also point out that most existing datasets involve only two participants, whereas most online discussions and deliberative platforms usually involve groups of people interacting with each other. 


\subsection{Large Language Models}
\label{sec:background:llm}

\acp{LLM} are sophisticated \ac{AI}-based computational models capable of text generation by training on vast amounts of written texts largely scrapped from the wider Internet. LLMs are based on the Transformer architecture \cite{vaswani2023attentionneed}, after it was widely adopted in numerous models undertaking many \ac{NLP} tasks. Without going into the history of how these models came to be, it is sufficient to say that LLMs used next-word-predictions to fulfill general tasks given by user-defined prompts. Because of their extensive size, complexity and pretraining, these models managed to compete with  previous specialized models in multiple tasks such as Topic Classification, Sentiment Analysis, Text Summarization \cite{ts2024}, as well as specialized annotation tasks \cite{tan2024largelanguagemodelsdata}.  Even more than that, they also proved capable of executing general tasks, leading to their worldwide use as personal assistants, automated systems, chatbots, and many more such roles. 

Another interesting property of LLMs is their ability to mimic human writing styles and interactions. Since a large part of their training data is sourced from social media (\href{https://www.reddit.com}{Reddit}, \href{https://www.twitter.com}{X (formerly Twitter)}, \href{https://www.facebook.com}{Facebook}, etc.), they often prove adept at participating seamlessly in human discussions. In fact, recent research \cite{Vezhnevets2023GenerativeAM, aher2023usinglargelanguagemodels} indicates that with proper prompting, LLMs can accurately mimic human writing having distinct subcultures, personalities and intents. Simulating general human behavior however is difficult, if not impossible; indeed, should this have been not been the case, human-involved studies would have become redundant.

Lastly, a common issue encountered with LLMs is that they tend to replicate toxic or inappropriate behaviors \cite{Birkun_Gautam_2023}, necessitating extensive and costly instruction tuning and \ac{RL} methods. In the context of synthetic discussions however, \textit{these faults are a feature, not a bug}, since toxic behaviors should be simulated in a realistic environment.

\section{Related Work}
\label{sec:related:sec2}

\subsection{LLM self-training}
\label{sec:related:self-train}

An area of intense LLM research is using LLMs to generate conversations among themselves. Researchers typically create a scenario where multiple agents discuss a given topic or a task, but instead of these agents being human, they are simulated by LLMs. The models are then finetuned on these conversations. Most approaches focus on strategies pitting a model against itself in an adversarial scenario \cite{liu2024largelanguagemodelsagents, cheng2024selfplayingadversariallanguagegame, zheng2024optimalllmalignmentsusing}, usually in the context of jailbreak evasion; jailbreaking being the formation of prompts which allow the model to generate harmful, illegal or explicit content. The results are then used to train the model via \ac{RL}. However, not all self-talking approaches use \ac{RL} or an adversarial scenario, nor are they used exclusively in the context of jailbreak prevention.

One of these \ac{RL} adapted techniques is \textit{"Self-play"}, where an agent learns by playing against itself rather than relying on a predefined set of opponents or scenarios. This method allows the agent to continually adapt and improve its strategies by facing progressively more challenging scenarios generated by its own evolving skills. Self-play has demonstrated spectacular results, outclassing human experts and rule-based computer algorithms in numerous games, the highest-profile being chess by the Alpha-Zero model in 2017 \cite{silver2017masteringchessshogiselfplay}. Self-play can be applied to LLMs by making them talk to each other \cite{cheng2024selfplayingadversariallanguagegame}.  \citet{ulmer2024bootstrappingllmbasedtaskorienteddialogue} propose a "Self-talk" framework where two LLMs are given roles ("client" and "agent") and a scenario which they act out. The client is given a personality and freedom to choose its actions, while the agent is restrained to a few actions depending on the client's actions. Specifically, both are given a prompt containing their role, personality, and dialogue history. The client is provided with an intention, while the agent with appropriate instructions. The researchers demonstrate that self-talk can indeed be used to improve LLMs, given enough finetuning and rigorous filtering of input data. Notably for our thesis, it provides a practical demonstration that LLMs conversing with each other can produce quality conversations when applied in a structured setting, even if they are ultimately not used for model finetuning.

Moreover, LLM self-play is hypothesized to work in discourse facilitation tasks. \citet{small-polis-llm} claim that synthetic data generation could be expanded to the scope of entire artificial discussions which, while not to be used to replace human interactions, can be very beneficial for testing and fine-tuning the system. This further solidifies the theoretical base of this thesis. 


\citet{abdelnabi2024cooperationcompetitionmaliciousnessllmstakeholders} focus on LLMs in multi-agent systems that work with hard negotiation tasks. The researchers model the negotiation process into a competitive, scorable game, involving six parties over five issues with multiple sub-options. Each actor in the negotiation is given a private summary of their stances on each issue (with attached scores), as well as general, public information about the other participants. It may also be given an intent; being cooperative, greedy or adversarial (trying to sabotage the negotiation). Each actor's success is quantified by the so-called scores of the parties and agreement thresholds, which need to be surpassed in order for an actor to be able to select an option. Finally, there is one role that holds ultimate veto power, although they are encouraged to use it only as a last result. The researchers note that the negotiation task itself is very difficult for most LLMs. We take inspiration from these experiments, and model one of our LLM prompting strategies after a competitive, scorable game.

It is important to note that conversations don't have to be constrained to only a few users. \citet{park2022socialsimulacracreatingpopulated} show a novel technique of populating entire communities with hundreds of members with a technique called "Social Simulacra". This technique allows a single LLM instance to use a community's description, rules, and a set of a few dozens personality types, to populate a virtual community with posts and comments made by hundreds of users, having diverse personalities, goals and motivations. The researchers show that appropriately prompted LLMs using generated personas are adequate at mimicking human users, their posts being generally indistinguishable from the mirrored actual communities to human annotators. The idea of automatically generating personas to be used in synthetic dialogues can be very beneficial for frameworks aiming at generating them, such as the one presented in this thesis.


Finally, \citet{lambert2024selfdirectedsyntheticdialoguesrevisions} follow the work of \citet{Bai2022ConstitutionalAH} and create a self-regulating conversation generation framework. Specifically, they use a set of given topics by \citet{Castricato2024SuppressingPE}, which include various principles fundamentally based on human rights. They then define various conversation goals (e.g. help a user create an email, an essay, perform language translation etc.). An LLM then generates a plan (system prompt) for the conversation and begins generating the conversation according to that plan, while checking if at any point the principles have been violated. In that case, it generates a critique on why the conversation failed. The models are encouraged to violate the goals of the conversation for the sake of data quality. It is worth noting that the study failed to find any trends between principles, goals and the generated text.

\subsection{LLMs bearing sociodemographic background}
\label{sec:related:sociodemographic}

Including a \ac{SDB} (race, age, ethnicity etc.) is a recent method frequently used in various \ac{NLP} tasks such as toxicity classification, hate speech detection and sentiment classification, although its efficacy is currently a matter of debate \cite{beck-etal-2024-sensitivity}. An interesting specialized area where this technique is used is in LLM prompting (\cite{hwang-etal-2023-aligning, durmus2024measuringrepresentationsubjectiveglobal} as cited by \citet{beck-etal-2024-sensitivity}), where sociodemographic prompting can reduce misunderstandings between people belonging to different social groups by carefully phrasing its output. 

\citet{beck-etal-2024-sensitivity} demonstrate that incorporating sociodemographic information into LLM prompts can significantly enhance performance in various subjective \ac{NLP} tasks under certain conditions. Large models (with over 11 billion parameters) often leverage combinations of sociodemographic traits rather than individual attributes, although they cannot treat these traits as explicit explanatory variables. However, this effect is highly dependent on factors such as prompt structure, model family, and model size, in ways that are not straightforward. Their findings thus support our hypothesis that incorporating user \acp{SDB} into LLM prompts can contribute to generating more diverse and realistic conversational outputs.

In addition to LLM sensitivities to sociodemographic prompts, the approach presents further limitations. Beyond the absence of standardized prompting templates and models capable of consistently leveraging sociodemographic information \cite{beck-etal-2024-sensitivity}, concerns have been raised regarding stereotypical biases \cite{cheng-etal-2023-marked, deshpande-etal-2023-toxicity}, as well as the strong orientation of models toward Western ideas and perspectives. There is also a lack of relevant datasets for languages other than English \cite{pmlr-v202-santurkar23a, durmus2024measuringrepresentationsubjectiveglobal, santy-etal-2023-nlpositionality} as cited by \citet{beck-etal-2024-sensitivity}. Additionally, \citet{aher2023usinglargelanguagemodels} report the issue of sociodemographic "distortions," where an LLMâ€™s responses and behavior diverge significantly from what might be expected from a human with the same \ac{SDB} context. For instance, an LLM simulating a human child might inaccurately include scientific knowledge, such as the melting point of aluminum, in its responses.


\subsection{LLMs as discourse facilitators}
\label{sec:related:discource}


LLMs are able to perform many facilitation tasks, which traditionally burdened human facilitators. One important use-case for LLMs is to iteratively summarize and refine the participants' understanding of the discussion and presented points. In a traditional discussion, a facilitator would present the participants with a summary of a key standpoint or worldview they presented as he understands it, and ask them whether the summary is correct \cite{small-polis-llm, Tsai2024Generative}. This procedure continues iteratively until the group believes that the facilitator understands them. These points can later be used by the facilitators during intergroup discussion in order to test hypotheses about the different groups' opinions, which is especially useful in finding common ground. It is hypothesized \cite{small-polis-llm} that using this procedure with an LLM may yield faster convergence to common ground and model understanding of the opinions of the participants. 

\citet{small-polis-llm} further point to using LLMs to directly produce opinions at the start of the dialogue (called "seed opinions" in the original paper) as another area of interest. However, \citet{karadzhov2023delidata} demonstrate that synthetic data are less convincing than retrieval-based, or even random selection of phrases from similar discussions, both on many metrics, and by human opinion. This phenomenon is more prevalent on issues which necessitate advanced vocabulary and reasoning.

\citet{al-khatib-etal-2018-modeling} analyze a deliberative discussion in terms of "deliberative strategies", which are comprised of a sequence of "moves" each participant can take during the discussion. We hypothesize that a LLM facilitator could look at the current state of discussion and recommend the best possible move according to the best possible strategy to the participant. It is worth noting that the researchers define the goal of a deliberative discussion differently than the one used by this paper and defined by Polis \cite{small-polis-llm}. Instead of the latter's definition being the civil and fair sharing of ideas, the researchers argue that a discussion leading to the "wrong action", or by reaching no agreement, has failed.

\citet{vecchi-2021-towards} report on human moderators and how their behavior should be modeled by automated systems. They provide an example where a moderator handles two users with different positions and argument styles who were in the process of derailing the discussion, and another where a user directly confronts the moderator on the definition of the forum's rules. Human moderators typically follow standard guidelines on how to approach situations such as these, as well as how to facilitate discussion, as discussed above. Thus, synthetic moderators should be modeled after these interactions and guidelines.

Finally, we note that LLMs are well positioned to tackle traditional \ac{NLP} problems relating to facilitating online discussions; namely machine translation (in order to allow marginalized and minority groups to contribute) \cite{Tsai2024Generative}, hate-speech \cite{Nirmal2024TowardsIH, shi-2024-hatespeech}, toxicity \cite{kang-qian-2024-implanting, Wang2022ToxicityDW} and fake-news \cite{Liu2024DetectIJ, Xu2024ACS} detection, in order to ensure effective moderation. 


\subsection{Measuring Discussion Quality}
\label{sec:related:measures}

\citet{vecchi-2021-towards} challenge the viewpoint that persuasiveness is a valid metric for judging an argument. They instead claim that an argument is useful when it either uncovers a previously hidden part of a problem, or combines and reconciles opposing views, advancing the discussion. The authors point to the Discourse Quality Index (DQI) \cite{Steiner2005-STEDPI-8, stab-gurevych-2017-parsing}, a metric developed by social scientists to properly analyze the quality of an argument. This index takes into consideration aspects such as respect, participation, interactivity and personal accounts and has a direct correlation with metrics used in NLP tasks \cite{wachsmuth-etal-2017-computational}. 

\citet{dekock2022disagree} point out that rebuttals usually lead to more constructive outcomes in a discussion. Their research additionally shows that dispute tactics are usually delivered in multiples; for example, credibility attacks are relatively rare, while credibility attacks combined with counterarguments or argument repetition are the respective two most observed tactics. Thus, a response may be both toxic and beneficial to the dialogue, provided it doesn't derail it by provoking other participants.

While the above criteria are certainly important for assessing the LLM \textit{moderator's} performance on actual conversations, we still lack a way of quantifying the quality/realism of the \textit{synthetic} dialogues. \citet{ulmer2024bootstrappingllmbasedtaskorienteddialogue} propose a series of automated evaluation metrics for synthetic dialogues. Non-task-specific metric include "Dialogue Diversity" which counts the number of n-grams (unigrams up to 5-grams) and the pairwise ROUGE-L \cite{lin-2004-rouge} score between the outputs of a LLM in a single interaction. Finally, "Character Consistency" measures how much the LLM stays in-character and is evaluated by a finetuned DeBERTa \cite{he2023debertav3improvingdebertausing} model. The researchers show a strong correlation between these metrics and subjective discussion quality evaluations. 

Ultimately, we conclude that there are no widespread, practical, computational metrics which can represent the quality of a discussion. Synthetic discussion quality metrics do exist, and are useful for filtering out low-quality generated dialogues during dataset preprocessing, but are not suitable for gauging the impact of LLM facilitators.


\subsection{Risks and Challenges}
\label{sec:related:challenges}

First, we feel compelled to echo the warnings of \citet{small-polis-llm} that synthetic data and conversations should by no means replace human content and interactions. This thesis builds a theoretical base for future frameworks, with models trained and tuned on LLM-to-LLM discussions, but deployed on human-to-human environments and monitored by human moderators. A harmful and dangerous use of this research could be the development of social-network troll/bot farms, as expressed by \citet{park2022socialsimulacracreatingpopulated}.

\citet{small-polis-llm} additionally outline several known weak points in LLM usage for moderation/facilitation; LLMs suffer from bias, hallucinations, are vulnerable to prompt injection attacks, and have their own political leanings (with most trending towards progressive ideas \cite{Taubenfeld2024SystematicBI}). Furthermore, \citet{vecchi-2021-towards} note that care must also be taken when quantifying argument quality by measures such as likes, to ensure the model does not discriminate against users who do not belong to a prevalent group or have difficulty communicating, as would be the case in frameworks such as Polis \cite{small2021polis}. They also recommend using discussions from online message boards for the initial synthetic comments ("seed opinions"). \citet{vecchi-2021-towards} however, warn of the challenges of sourcing such comments; personal opinions, facts and fake news are often bundled together in online discussions.

Lastly, training generative models, and more specifically LLMs, on their own data most often leads to the model collapsing \cite{alemohammad2023selfconsuminggenerativemodelsmad, shumailov2024curserecursiontraininggenerated} as cited by \citet{ulmer2024bootstrappingllmbasedtaskorienteddialogue}. Even when not trained on their own data, LLMs tasked with creating dialogues often generate low quality, off-topic and generally useless conversations.In their experiments, \citet{ulmer2024bootstrappingllmbasedtaskorienteddialogue} show that, at many points, the conversation collapses. Their actors in this case go off-script, begin rambling or end the interaction too early or too late. Other challenges include hard and soft errors when generating data at-scale \cite{lambert2024selfdirectedsyntheticdialoguesrevisions, ulmer2024bootstrappingllmbasedtaskorienteddialogue} requiring automatic verification steps, insidious errors which can not be reasonably caught by automated metrics \cite{lambert2024selfdirectedsyntheticdialoguesrevisions, ulmer2024bootstrappingllmbasedtaskorienteddialogue}, and a lack of generated topic diversity \cite{lambert2024selfdirectedsyntheticdialoguesrevisions}.


\subsection{Related Datasets}
\label{sec:related:datasets}

Synthetic-only dialogue datasets are exceedingly rare in literature. \cite{lambert2024selfdirectedsyntheticdialoguesrevisions} provide a dataset containing  $108,000$ sentences generated by different models, using a topic, subtopic and goal for each conversation. They also publish a sister dataset containing the LLM annotations for why the conversation violated the stated policies of the discussion. Thus, we need to explore datasets from adjacent tasks to aid us in analyzing and evaluating our own discussions in the future.

One of the most frequently used datasets for conversation escalation analysis is the "Wikipedia Disputes" dataset \cite{de-kock-vlachos-2021-beg}, which contains discussions from Wikipedia's talk pages, where members attempt to resolve edit disputes. The annotated labels correspond to whether a dispute "escalated", meaning that the members could not resolve it by themselves, and thus requested moderator arbitration. \citet{dekock2022disagree} build upon their work and provide the "WikiTactics" dataset, which provides annotations based on the tactics employed in each comment. \citet{hua2018wikiconvcorpuscompleteconversational} enhance the "Wikipedia Disputes" dataset, including metadata concerning edits, deletions and other actions on the comments themselves. This approach was followed by \citet{al-khatib-etal-2018-modeling} who provide a large-scale dataset generated from Wikipedia discussions, called "Webis-WikiDebate-18 corpus", designed to model deliberative discourse based on metadata categories. The dataset contains $2,400$ turns labeled with discourse acts, $7,437$ turns labeled with relational connections between comments, and $182,321$ turns labeled with discourse frames. Each turn in the discussion is labeled automatically using metadata that corresponds to specific discourse categories derived from their own discourse classification models.

Early conversation derailment datasets are also available, albeit in relatively small numbers. These datasets are useful for diagnosing the causes of conversational collapse in human dialogues. \citet{zhang-2018-gone-awry} provide a curated dataset of $1,270$ conversations with an average length of $4.6$ comments each, featuring derailed conversations. \citet{chang-danescu-niculescu-mizil-2019-trouble} provide two datasets relating to discussion derailment, the first expanding on the previous dataset with a total size of $4,188$ conversations and a larger discussion length, while the second is sourced from the "Change My View" (CMV) Subreddit, featuring $600,000$ conversations, $6,842$ of which necessitated moderator intervention.
