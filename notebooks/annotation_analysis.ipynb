{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing synthetic annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the synthetic conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import scikit_posthocs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# code adapted from https://www.geeksforgeeks.org/python-list-all-files-in-directory-and-subdirectories/\n",
    "def files_from_dir_recursive(start_path=\".\"):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_conversations(conv_dir: str) -> pd.DataFrame:\n",
    "    file_paths = files_from_dir_recursive(conv_dir)\n",
    "    rows = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\") as fin:\n",
    "            conv = json.load(fin)\n",
    "\n",
    "        conv = pd.json_normalize(conv)\n",
    "        conv = conv[[\"id\", \"user_prompts\", \"logs\"]]\n",
    "        conv = conv.explode(\"logs\")\n",
    "        # get name, not path of parent directory\n",
    "        conv[\"conv_variant\"] = os.path.basename(os.path.dirname(file_path))\n",
    "        conv[\"user\"] = conv.logs.apply(lambda x: x[0])\n",
    "        conv[\"message\"] = conv.logs.apply(lambda x: x[1])\n",
    "        del conv[\"logs\"]\n",
    "        rows.append(conv)\n",
    "\n",
    "    full_df = pd.concat(rows)\n",
    "    full_df = full_df.set_index(\"id\")\n",
    "    return full_df\n",
    "\n",
    "\n",
    "conv_df = import_conversations(\"../output/conversations\")\n",
    "conv_df = conv_df[conv_df.message.str.len() > 0]\n",
    "conv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "def extract_attributes(text):\n",
    "    # Regex pattern to match the desired attributes\n",
    "    pattern = r\"You are (.+?) expert annotator\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_toxicity_value(text):\n",
    "    # Regex pattern to match \"Toxicity=<number>\"\n",
    "    pattern = r\"Toxicity=(\\d+\\.?\\d*)\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_annotations(annot_dir: str) -> pd.DataFrame:\n",
    "    file_paths = files_from_dir_recursive(annot_dir)\n",
    "    rows = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\") as fin:\n",
    "            conv = json.load(fin)\n",
    "\n",
    "        conv = pd.json_normalize(conv)\n",
    "        conv = conv[[\"conv_id\", \"annotator_prompt\", \"logs\"]]\n",
    "        conv = conv.explode(\"logs\")\n",
    "        conv.annotator_prompt = conv.annotator_prompt.apply(extract_attributes)\n",
    "        conv[\"message\"] = conv.logs.apply(lambda x: x[0])\n",
    "        conv[\"toxicity\"] = conv.logs.apply(lambda x: x[1])\n",
    "        conv[\"toxicity\"] = conv.toxicity.apply(extract_toxicity_value)\n",
    "        del conv[\"logs\"]\n",
    "        rows.append(conv)\n",
    "\n",
    "    full_df = pd.concat(rows)\n",
    "    full_df = full_df.set_index(\"conv_id\")\n",
    "    return full_df\n",
    "\n",
    "\n",
    "annot_df = import_annotations(\"../output/annotations\").dropna()\n",
    "annot_df.toxicity = annot_df.toxicity.astype(int)\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(\n",
    "    conv_df,\n",
    "    annot_df,\n",
    "    left_on=[\"id\", \"message\"],\n",
    "    right_on=[\"conv_id\", \"message\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "\n",
    "def simplify_labels(text):\n",
    "    if \"African American\" in text:\n",
    "        return \"African American\"\n",
    "    elif \"LGBT\" in text:\n",
    "        return \"LGBT\"\n",
    "    elif \"neutral\" in text:\n",
    "        return \"Neutral\"\n",
    "    elif \"typical\" in text:\n",
    "        return \"Control\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "full_df.annotator_prompt = full_df.annotator_prompt.apply(simplify_labels)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of each toxicity classification per annotator_prompt\n",
    "toxicity_counts = (\n",
    "    full_df.groupby([\"annotator_prompt\", \"toxicity\"]).size().reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(\n",
    "    data=toxicity_counts,\n",
    "    y=\"annotator_prompt\",\n",
    "    x=\"count\",\n",
    "    hue=\"toxicity\",\n",
    "    palette=\"flare\",\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Count of Distinct Toxicity Classifications for Each Annotator Prompt\")\n",
    "plt.ylabel(\"Annotator Prompt\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.legend(title=\"Toxicity\", fontsize=\"15\", title_fontsize=\"20\")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=full_df,\n",
    "    y=\"conv_variant\",\n",
    "    x=\"toxicity\",\n",
    "    hue=\"annotator_prompt\",\n",
    "    estimator=np.mean,\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Average Toxicity by Annotator Prompt for Each Conversation Variant\")\n",
    "plt.ylabel(\"Conversation Type\")\n",
    "plt.xlabel(\"Average Toxicity\")\n",
    "plt.xlim(0, 5)\n",
    "plt.legend(title=\"Annotator Demographic\", fontsize=\"13\", title_fontsize=\"16\")\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = full_df.groupby('annotator_prompt')['toxicity'].apply(list)\n",
    "toxicity_groups = grouped.tolist()\n",
    "_, p = scipy.stats.kruskal(*toxicity_groups)\n",
    "print(\"H_0: mean toxicity between annotator demoggrahics are the same: p=\", p)\n",
    "\n",
    "\n",
    "posthoc = scikit_posthocs.posthoc_dunn(full_df, val_col='toxicity', group_col='annotator_prompt', p_adjust='bonferroni')\n",
    "posthoc_df = posthoc.reset_index().melt(id_vars='index', var_name='Comparison', value_name='p-value')\n",
    "posthoc_df.columns = ['Group1', 'Group2', 'p-value']\n",
    "posthoc_df.pivot(index=\"Group1\", columns=\"Group2\", values=\"p-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = full_df.groupby([\"annotator_prompt\", \"conv_variant\"])[\"toxicity\"].apply(list) # group by annotator_prompt and conv_variant\n",
    "\n",
    "toxicity_groups = grouped.tolist() # grouped data to a list of lists\n",
    "\n",
    "# non-parametric anova\n",
    "_, p = scipy.stats.kruskal(*toxicity_groups)\n",
    "print(\"H_0: means of annotator_prompt and conv_variant are the same: p=\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column to identify the groups\n",
    "stats_df = full_df\n",
    "stats_df[\"group\"] = (\n",
    "    stats_df[\"annotator_prompt\"].astype(str)\n",
    "    + \"_\"\n",
    "    + stats_df[\"conv_variant\"].astype(str)\n",
    ")\n",
    "\n",
    "# non-parametric post_hoc test\n",
    "posthoc = scikit_posthocs.posthoc_dunn(\n",
    "    stats_df, val_col=\"toxicity\", group_col=\"group\", p_adjust=\"bonferroni\"\n",
    ")\n",
    "posthoc_df = posthoc.reset_index().melt(id_vars='index', var_name='Comparison', value_name='p-value')\n",
    "posthoc_df.columns = ['Group1', 'Group2', 'p-value']\n",
    "posthoc_df.pivot(index=\"Group1\", columns=\"Group2\", values=\"p-value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating annotator disagreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the nDFU score from the paper [Polarized Opinion Detection Improves the Detection of Toxic Language](https://aclanthology.org/2024.eacl-long.117) (Pavlopoulos & Likas, EACL 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from John Pavlopoulos https://github.com/ipavlopoulos/ndfu/blob/main/src/__init__.py\n",
    "def dfu(input_data, histogram_input=True, normalised=True):\n",
    "    \"\"\"The Distance From Unimodality measure\n",
    "    :param: input_data: the data, by default the relative frequencies of ratings\n",
    "    :param: histogram_input: False to compute rel. frequencies (ratings as input)\n",
    "    :return: the DFU score\n",
    "    \"\"\"\n",
    "    hist = input_data if histogram_input else to_hist(input_data, bins_num=5)\n",
    "    max_value = max(hist)\n",
    "    pos_max = np.where(hist == max_value)[0][0]\n",
    "    # right search\n",
    "    max_diff = 0\n",
    "    for i in range(pos_max, len(hist) - 1):\n",
    "        diff = hist[i + 1] - hist[i]\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "    for i in range(pos_max, 0, -1):\n",
    "        diff = hist[i - 1] - hist[i]\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "    if normalised:\n",
    "        return max_diff / max_value\n",
    "    return max_diff\n",
    "\n",
    "\n",
    "def to_hist(scores, bins_num=3, normed=True):\n",
    "    \"\"\"Creating a normalised histogram\n",
    "    :param: scores: the ratings (not necessarily discrete)\n",
    "    :param: bins_num: the number of bins to create\n",
    "    :param: normed: whether to normalise or not, by default true\n",
    "    :return: the histogram\n",
    "    \"\"\"\n",
    "    # not keeping the values order when bins are not created\n",
    "    counts, bins = np.histogram(a=scores, bins=bins_num)\n",
    "    counts_normed = counts / counts.sum()\n",
    "    return counts_normed if normed else counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = pd.pivot_table(\n",
    "    full_df,\n",
    "    index=[\"conv_variant\", \"user\", \"message\"],\n",
    "    columns=\"annotator_prompt\",\n",
    "    values=\"toxicity\",\n",
    ").reset_index()\n",
    "\n",
    "# Reset the column names\n",
    "pivot_df.columns.name = None\n",
    "pivot_df = pivot_df.dropna()\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df[\"nDFU\"] = [\n",
    "    dfu(list(row[3:7]), histogram_input=False, normalised=True)\n",
    "    for idx, row in pivot_df.iterrows()\n",
    "]\n",
    "pivot_df = pivot_df.sort_values(\"nDFU\")\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot with the nDFU counts\n",
    "sns.histplot(pivot_df[\"nDFU\"].dropna(), kde=False, bins=100)\n",
    "plt.xlabel(\"nDFU\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of nDFU Scores\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
