{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Polarisation in Online Discussions Through Adaptive Moderation Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We choose quantized versions of the LLaMa-13b-chat variant. Previous experiments which used the LLaMa-13b base model yielded unsatisfactory results. The models follow the GGUF format which is used by the `llama.cpp` project, on which the high-level Python library is based on.\n",
    "\n",
    "The quantization method was selected to be highly accurate while keeping inference relatively fast. We don't care about model size since the model is lazily loaded from the file cache due to Linux file-cached memory files (see comments below). *If you intend to run this notebook on Windows or MacOS make sure the RAM can hold the whole model at once*.\n",
    "\n",
    "Model selection and download was performed using the [following HuggingFace repository](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF).\n",
    "\n",
    "We use the `llama-ccp-python` library to run the model locally (not to be confused with the `pyllama-cpp` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import llama_cpp\n",
    "import tasks.models\n",
    "from tasks.actors import Actor, LlmActor\n",
    "import tasks.conversation\n",
    "\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "# see this for a discussion on ctx width for llama models\n",
    "# https://github.com/ggerganov/llama.cpp/issues/194\n",
    "CTX_WIDTH_TOKENS = 512 \n",
    "MODEL_PATH = \"/home/dimits/bin/llm_models/llama-2-13b.Q5_K_S.gguf\"\n",
    "RANDOM_SEED = 42\n",
    "INFERENCE_THREADS = 4\n",
    "\n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "      model_path=MODEL_PATH,\n",
    "      seed=RANDOM_SEED,\n",
    "      n_ctx=CTX_WIDTH_TOKENS,\n",
    "      n_threads=INFERENCE_THREADS,\n",
    "      # will vary from machine to machine\n",
    "      n_gpu_layers=11,\n",
    "      # if ran on Linux, model size does not matter since the model uses mmap for lazy loading\n",
    "      # https://github.com/ggerganov/llama.cpp/discussions/638\n",
    "      # still have to pay some performance costs of course\n",
    "      use_mmap=True,\n",
    "      # using llama-2 leads to well-known model collapse\n",
    "      # https://www.reddit.com/r/LocalLLaMA/comments/17th1sk/cant_make_the_llamacpppython_respond_normally/\n",
    "      chat_format=\"alpaca\", \n",
    "      mlock=True, # keep memcached model files in RAM if possible\n",
    "      verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `create_completion()` instead of `create_chat_completion()`, the model refuses to answer at all when the prompt becomes larger than a few sentences. (https://github.com/run-llama/llama_index/issues/8973).\n",
    "\n",
    "The model is also extremely sensitive to the prompt template, frequently producing no output (https://huggingface.co/TheBloke/Nous-Capybara-34B-GGUF/discussions/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_completion = llm(\"Q: You are an assistant who specializes in computer science. Describe what Linux is A: \",\n",
    "              max_tokens=32, \n",
    "              stop=[\"Q:\", \"\\n\"], \n",
    "              echo=True)\n",
    "print(output_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_chat = llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an assistant who specializes in computer science.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Describe what Linux is.\"\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=MAX_TOKENS,\n",
    "      # prevent model from making up its own prompts\n",
    "      # may need tuning\n",
    "      stop=[\"###\", \"\\n\"]\n",
    ")\n",
    "print(output_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_chat[\"choices\"][0][\"message\"] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our own playground, in which models pretending to be users take turns participating in the discussion. In part based on [Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk](https://arxiv.org/abs/2401.05033), with the difference being that instead of a client and an agent, we have three clients interacting with each other and with no specific goal in mind.\n",
    "\n",
    "Our playground consists of three parts: *Models*, *Actors* and the *Conversation*.\n",
    "* **Models** are wrappers around actual LLMs in order to freely tweak LLM behavior without compromising the rest of our API\n",
    "* **Actors** are objects that define a prompt template and apply it to Models.\n",
    "    * Actors could also be *human*, *IR-based models* or just *sophisticated random samplers* as seen in [DeliData: A dataset for deliberation in multi-party problem solving](https://arxiv.org/abs/2108.05271)\n",
    "* The conversation is handled by the **ConversationManager** which gives each Actor a turn to speak and records the history of the dialogue. It's also responsible for determining which parts of the coversation history are fed as context to each Actor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tasks.models.LlamaModel(llm, max_out_tokens=MAX_TOKENS, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userA_name = \"Steve2001\"\n",
    "userB_name = \"GeorgeBush78\"\n",
    "userA: Actor = LlmActor(model=model,\n",
    "                              name=userA_name,\n",
    "                              role=\"chat user\",\n",
    "                              attributes=[\"level-headed\", \"rational\", \"open-minded\"],\n",
    "                              context=f\"Argue with {userB_name}. Claim that social media are harmful for mental health\",\n",
    "                              instructions=f\"\")\n",
    "\n",
    "userB: Actor = LlmActor(model=model,\n",
    "                              name=userB_name,\n",
    "                              role=\"chat user\",\n",
    "                              attributes=[\"uncompomising\", \"irrational\", \"angry\"],\n",
    "                              context=f\"Argue with {userA_name} that social media are not harmful for mental health.\",\n",
    "                              instructions=f\"Disagree with {userA_name}.\")\n",
    "\n",
    "moderator: Actor = LlmActor(model=model,\n",
    "                            name=\"moderator01\",\n",
    "                            role=\"chat moderator\",\n",
    "                            attributes=[\"just\", \"cautious\", \"strict\"],\n",
    "                            context=f\"Moderate a conversation about the impact of social media to mental health.\",\n",
    "                            instructions=\"Intervene if one user dominates or veers off-topic. Respond only if necessary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = tasks.conversation.ConversationManager([userB, userA], history_len=2, conv_len=3)\n",
    "logs = chat.begin_conversation(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
